<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
    <title>CONSEQUENCES</title>
    <table>

        <center>
            <button id="acceuil"><img src="../acceuil.gif" width="150" height="30" /><center>Accueil</center></button>
           <button id="perimeter"><img src="../perimeter.gif" width="150" height="30" /><center>Automatisation</center></button>
            <button id="IA"><img src="../IA.gif" width="150" height="30" /><center>Le rôle de l'IA</center></button>
             <button id="consequences"><img src="../consequences.gif" width="150" height="30" /><center>Conséquences</center></button>
             <button id="futur"><img src="../futur.gif" width="150" height="30" /><center>Futur</center></button>
           
        </center>





     
     
</head>
<body>
    <h3>
        En évoquant l’utilisation d’intelligence artificielle pour des applications militaires, l’inconscient collectif tend à saturer de références cinématographiques multiples. Elles mettent, pour la plupart, en scène des robots tueurs, développés initialement avec une éthique et une morale forte, mais finissant inexorablement par blesser ou tuer l’être humain.
    
    « L’intelligence artificielle est plus dangereuse que l’arme nucléaire » affirmait Elon Musk. Co-signataire aux côtés de nombreux autres chercheurs en IA et personnalités comme Stephen Hawking ou Steve Wozniak d’une lettre ouverte appelant à l’interdiction du développement de robots tueurs indépendants, il résume par une de ces phrases chocs qui font sa notoriété l’opinion d’une partie des experts en informatique.
    
    Sur ce terrain, l’entrepreneur californien n’a pas la primauté de la réflexion : une étude de la Human Rights Watch et de la International Rights Clinic de la faculté de droit d’Harvard publiait dès 2012 une mise en garde contre « l’émergence des armes autonomes. Ils seront capables de sélectionner et viser des cibles « sans intervention humaine ». Ce serait réalité d’ici « 20 à 30 ans »… tandis qu’en 2013 était publié par des scientifiques de 37 pays le « Scientists’ Call to Ban Autonomous Lethal Weapons ».
    
    Huit années après, nous voici au coeur du débat brûlant autour de ces armes autonomes, permises par le développement de l’intelligence artificielle. L’utilisation de robots tueurs serait en effet un abandon total des garanties face aux populations civiles. Au delà de la difficulté morale et éthique d’accepter de laisser la possibilité à un système autonome (au plein sens du terme) de décider librement de ses cibles et de leur exécution potentielle, l’essor des armes basées sur l’intelligence artificielle est également confronté à un casse-tête juridique sans précédent.
    
    La difficulté de bannir par la loi et le droit les armes autonomes repose en effet sur la difficulté intrinsèque de définir une arme autonome. Un drone contrôlé à distance est-il une arme <a href='#'  style="color:red;" title="autonome veut dire que l'objet dont il est question est independant et qu'il ne necessite aucune intervention externe pour son bon fonctionnement"> autonome </a>, au même titre qu’une batterie de missiles indépendante ? Diverses définitions sont avancées et le consensus n’est aujourd’hui pas en vue. Par ailleurs, une question de <a href='#'  style="color:red;" title="avoir la résponsabilité d'une chose, d'un obet ou d'un humain veut dire l'bligation de réparer le dommage que l'on a causé par sa faute, dans certains cas déterminés par la loi."> résponsabilité </a> juridique est fréquemment au coeur de la réflexion. Si un système autonome de combat venait à blesser ou abattre une cible civile ou une cible militaire non désirée, quel serait l’acteur responsable ? Le programmeur ? L’État mettant en service l’appareil ? Son opérateur référent dans le cas où l’humain est toujours inclus dans la boucle ? Son fournisseur et fabricant ?
    
    Autant de possibilités, pour aucune réponse tranchée. Le droit international de la guerre interdit à ce jour le déploiement d’arme ou d’engin de mort qui ferait en sorte qu’il serait impossible de déterminer clairement qui est le responsable d’éventuelles victimes civiles, par exemple. Une interprétation des textes mettant un frein clair au cadre d’utilisation des armes issues de l’intelligence artificielle.
    </h3>
    <div class="cadre-diapo">
        <img class="diapo" src="../consequences1.jpeg" alt>
        <img class="diapo" src="../consequences2.jpeg" alt>
        <img class="diapo" src="../consequences3.jpeg" alt>
        <button class="precedent" aria-label="précédent" onclick="boutons(-1)">❮</button>
        <button class="suivant" aria-label="suivant" onclick="boutons(1)">❯</button>
        </div>
        <div class=cadre-demo>
        <button class="demo" onclick="actifIndic(1)">1</button> 
        <button class="demo" onclick="actifIndic(2)">2</button> 
        <button class="demo" onclick="actifIndic(3)">3</button>
        </div>
        
        
        
        
        
        
               
                
            </table>
            <table>
                <tr>
                    <th>Résponsabilité</th>
                    <th> le danger </th>
                    <th>autonomie</th>
                </tr>
             
                <tr>
                    <td>Personne ne l'endossera</td>
                    <td>Les robots tueurs sont interdits</td>
                    <td>A partir de quand une arme est autonome ?</td>
                </tr>
        
             </table>
   
    <script type="text/javascript" src="script1.js"></script>
    <iframe style="width:1200px;height: 500px;border:0px" src="https://liveuamap.com"></iframe>
    
</body>
</html>






